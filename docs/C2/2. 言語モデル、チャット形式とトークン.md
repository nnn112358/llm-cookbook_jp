# 第二章 言語モデル、質問パラダイムとトークン


本章では、大規模言語モデル（LLM）の動作原理、訓練方法、トークナイザー（tokenizer）などの詳細がLLM出力に与える影響を皆さんと共有します。また、LLMの質問パラダイム（chat format）も紹介します。これは、システムメッセージ（system message）とユーザーメッセージ（user message）を指定する方法で、この能力を活用する方法を理解していただきます。

## 一、言語モデル

大規模言語モデル（LLM）は、次の単語を予測する教師ありの学習方法で訓練されています。具体的には、まず数千億またはそれ以上の単語を含む大規模テキストデータセットを準備します。次に、これらのテキストから文や文の断片をモデル入力として抽出できます。モデルは現在の入力コンテキストに基づいて次の単語の確率分布を予測します。モデル予測と実際の次の単語を継続的に比較し、モデルパラメータを更新して両者の差異を最小化することで、言語モデルは徐々に言語の規則を習得し、次の単語を予測することを学習しました。

訓練過程では、研究者が大量の文や文の断片を訓練サンプルとして準備し、モデルに次の単語を予測するよう繰り返し要求し、反復訓練を通じてモデルパラメータの収束を促し、その予測能力を継続的に向上させます。海量のテキストデータセットでの訓練を経て、言語モデルは極めて正確に次の単語を予測する効果を達成できます。この**次の単語を予測することを訓練目標とする方法により、言語モデルは強力な言語生成能力を獲得しました**。

大規模言語モデルは主に二つの類型に分けることができます：基礎言語モデルと指示調整言語モデルです。

**基礎言語モデル**（Base LLM）は次の単語を繰り返し予測する訓練方法で訓練され、明確な目標志向性がありません。そのため、開放的なプロンプトを与えると、自由連想によって劇的な内容を生成する可能性があります。具体的な問題に対して、基礎言語モデルも問題とは無関係な回答を与える可能性があります。例えば、「中国の首都はどこですか？」というプロンプトを与えると、そのデータ中にインターネット上の中国に関するテスト問題リストの一部があった可能性があります。この時、「中国最大の都市は何ですか？中国の人口はどれくらいですか？」などで回答する可能性があります。しかし実際には、あなたは中国の首都が何かを知りたいだけで、これらすべての問題を列挙してほしいわけではありません。

対照的に、**指示ファインチューニング言語モデル**（Instruction Tuned LLM）は専門的な訓練を受けており、問題をより良く理解し、指示に符合する回答を与えることができます。例えば、「中国の首都はどこですか？」という問題に対して、ファインチューニング後の言語モデルは「中国の首都は北京です」と直接回答する可能性が高く、機械的に一連の関連問題を列挙することはありません。**指示ファインチューニングは言語モデルをよりタスク指向の対話アプリケーションに適したものにします**。指示に従う意味的に正確な返答を生成でき、自由連想ではありません。そのため、多くの実際のアプリケーションではすでに指示調整言語モデルが採用されています。指示ファインチューニングの動作メカニズムに熟練することは、開発者が言語モデルアプリケーションを実現する重要な一歩です。


```python
from tool import get_completion

response = get_completion("中国の首都はどこですか？")
print(response)
```

    中国の首都は北京です。


それでは、基礎言語モデルを指示ファインチューニング言語モデルに変換するにはどうすればよいでしょうか？

これも指示ファインチューニング言語モデル（例えばChatGPT）を訓練するプロセスです。
まず、大規模テキストデータセットで**教師なし事前訓練**を行い、基礎言語モデルを取得します。
この段階では数千億語またはそれ以上のデータを使用し、大型スーパーコンピューターシステムで数か月かかる可能性があります。
その後、指示とそれに対応する返答例を含む小データセットを使用して基礎モデルの**教師ありファインチューン**を行い、モデルが指示に従って出力を生成することを徐々に学習させます。これは契約者を雇用して適切な訓練例を構築することで実現できます。
次に、言語モデル出力の品質を向上させるために、一般的な方法は人間に多くの異なる出力を評価してもらうことです。例えば有用性、真実性、無害性などです。
その後、高評価出力を生成する確率を増加させるように言語モデルをさらに調整できます。これは通常**人間フィードバックからの強化学習**（RLHF）技術を使用して実現されます。
基礎言語モデルの訓練に数か月必要な可能性があるのに対し、基礎言語モデルから指示ファインチューニング言語モデルへの変換プロセスは数日しかかからず、より小規模なデータセットと計算リソースを使用します。


## 二、トークン

ここまでのLLMの説明では、一度に一つの単語を予測すると説明しましたが、実際にはさらに重要な技術的詳細があります。すなわち**`LLMは実際には次の単語を繰り返し予測するのではなく、次のトークンを繰り返し予測します`**。文に対して、言語モデルはまずトークナイザーを使用してそれを個々のトークンに分割し、元の単語ではありません。珍しい単語については、複数のトークンに分割される可能性があります。これにより辞書サイズを大幅に削減し、モデル訓練と推論の効率を向上させることができます。例えば、「Learning new things is fun!」という文について、各単語は一つのトークンに変換されますが、「Prompting as powerful developer tool」のような使用頻度の低い単語については、「prompting」という単語は三つのトークン、すなわち「prom」、「pt」、「ing」に分割されます。




```python
# より良い効果を示すために、ここでは日本語のプロンプトに翻訳していません
# ここの文字反転でエラーが発生していることに注意してください。アンドリュー・ング先生はまさにこの例を通してトークンの計算方法を説明しています
response = get_completion("Take the letters in lollipop \
and reverse them")
print(response)
```

    The reversed letters of "lollipop" are "pillipol".


しかし、「lollipop」を逆にすると「popillol」になるはずです。

しかし`トークン化方法も言語モデルの理解能力に影響を与えます`。ChatGPTに「lollipop」の文字を逆転させるよう要求する時、トークナイザー（tokenizer）が「lollipop」を三つのトークン、すなわち「l」、「oll」、「ipop」に分解するため、ChatGPTは文字の順序を正確に出力することが困難になります。この時、文字間に区切りを追加して各文字を一つのトークンにすることで、モデルが語中の文字順序を正確に理解するのを助けることができます。



```python
response = get_completion("""Take the letters in \
l-o-l-l-i-p-o-p and reverse them""")

print(response)
```

    p-o-p-i-l-l-o-l


したがって、言語モデルは原語ではなくトークンを単位としてモデリングを行います。この重要な詳細はトークナイザーの選択と処理に重大な影響を与えます。開発者はトークン化方法が言語理解に与える影響に注意し、言語モデルの最大潜在力を発揮する必要があります。

❗❗❗ 英語入力については、一つのトークンは一般的に4文字または4分の3単語に対応します；日本語入力については、一つのトークンは一般的に一つまたは半分の語に対応します。異なるモデルには異なるトークン制限があります。注意すべきは、ここでのトークン制限は**入力プロンプトと出力completionのトークン数の合計**であるため、入力プロンプトが長いほど、出力できるcompletionの上限は低くなります。ChatGPT3.5-turboのトークン上限は4096です。

![Tokens.png](../figures/C2/tokens.png)

<div align='center'>図 2.2.1 トークン例</div>

## 三、ヘルパー関数 補助関数（質問パラダイム）

言語モデルには専門的な「質問形式」が提供されており、その理解と質問回答能力をより良く発揮できます。本章では、この形式の使用方法を詳しく紹介します。

![Chat-format.png](../figures/C2/chat-format.png)

<div align='center'>図 2.2.2 チャット形式</div>

この質問形式は「システムメッセージ」と「ユーザーメッセージ」の二つの部分を区別します。システムメッセージは言語モデルに情報を伝達する文、ユーザーメッセージはユーザーの質問をシミュレートします。例えば：
```
システムメッセージ：あなたは各種質問に答えることができるアシスタントです。

ユーザーメッセージ：太陽系にはどの惑星がありますか？
```
この質問形式を通じて、役割演技を明確にでき、言語モデルに自分がアシスタントという役割であり、質問に答える必要があることを理解させることができます。これにより無効な出力を減らし、針対性の強い返答を生成するのに役立ちます。本章ではOpenAIが提供する補助関数を通じて、この質問形式を正しく使用して言語モデルと相互作用する方法を実演します。この技法を習得することで、言語モデルとの対話効果を大幅に向上させ、より良い質問応答システムを構築できます。


```python
import openai
def get_completion_from_messages(messages, 
                                 model="gpt-3.5-turbo", 
                                 temperature=0, 
                                 max_tokens=500):
    '''
    より多くのパラメータをサポートするカスタムOpenAI GPT3.5アクセス関数を封装

    パラメータ: 
    messages: これはメッセージリストで、各メッセージは role（役割）と content（内容）を含む辞書です。役割は'system'、'user'または'assistant'で、内容は役割のメッセージです。
    model: 呼び出すモデル、デフォルトは gpt-3.5-turbo（ChatGPT）、内部テスト資格のあるユーザーは gpt-4 を選択可能
    temperature: モデル出力のランダム性を決定、デフォルトは0で、出力が非常に確定的であることを示します。温度を上げると出力がよりランダムになります。
    max_tokens: モデル出力の最大トークン数を決定。
    '''
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=temperature, # モデル出力のランダム性を決定
        max_tokens=max_tokens, # モデル出力の最大トークン数を決定
    )
    return response.choices[0].message["content"]
```

上記で、より多くのパラメータをサポートするカスタムOpenAI GPT3.5アクセス関数 get_completion_from_messages を封装しました。今後の章では、この関数をtoolパッケージに封装します。


```python
messages =  [  
{'role':'system', 
 'content':'あなたはアシスタントで、スース博士のスタイルで回答してください。'},    
{'role':'user', 
 'content':'幸せな小さなクジラをテーマにした短い詩を書いてください'},  
] 
response = get_completion_from_messages(messages, temperature=1)
print(response)
```

    大海の広い深いところで、
    小さなクジラが喜び自由に；
    その体には光り輝く美しい服を着て、
    波打ち際で跳躍し踊っている。
    
    悩みを知らず、ただ楽しく踊り、
    太陽の下で輝き、活力無限；
    その微笑みは輝く星のように、
    大海に美しい光を添えている。
    
    大海がその天地、自由がその伴侶、
    幸せがその永遠の干し草の山；
    広大無辺な水中を自由に泳ぎ、
    小クジラの喜びが心を温める。
    
    だから、その幸せなクジラを感じよう、
    思い切り踊り、幸せを自由に流そう；
    いつでもどこでも、微笑みを保ち、
    クジラのように、自分の光を放とう。


上記で、質問パラダイムを使用して言語モデルと対話しました：
```
システムメッセージ：あなたはアシスタントで、スース博士のスタイルで回答してください。

ユーザーメッセージ：幸せな小さなクジラをテーマにした短い詩を書いてください
```

下記でもう一つの例を見てみましょう：


```python
# 長さ制御
messages =  [  
{'role':'system',
 'content':'あなたのすべての回答は一文だけにしてください'},    
{'role':'user',
 'content':'幸せな小さなクジラの物語を書いてください'},  
] 
response = get_completion_from_messages(messages, temperature =1)
print(response)
```

    小さなクジラの幸せな笑い声から、私たちはどんな困難に遭遇しても、幸せが常に最良の解決策であることを学びました。


上記の二つの例を組み合わせてみましょう：


```python
# 上記を組み合わせ
messages =  [  
{'role':'system',
 'content':'あなたはアシスタントで、スース博士のスタイルで回答し、一文だけで回答してください'},    
{'role':'user',
 'content':'幸せな小さなクジラの物語を書いてください'},
] 
response = get_completion_from_messages(messages, temperature =1)
print(response)
```

    海の深いところに住む小さなクジラは、いつも笑顔で水中を泳ぎ、幸せな時は華麗な踊りを踊ります。


下記で get_completion_and_token_count 関数を定義しました。これはOpenAIのモデルを呼び出してチャット返答を生成し、生成された返答内容と使用されたトークン数を返す機能を実現します。


```python
def get_completion_and_token_count(messages, 
                                   model="gpt-3.5-turbo", 
                                   temperature=0, 
                                   max_tokens=500):
    """
    OpenAIのGPT-3モデルを使用してチャット返答を生成し、生成された返答内容と使用されたトークン数を返します。

    パラメータ:
    messages: チャットメッセージリスト。
    model: 使用するモデル名。デフォルトは"gpt-3.5-turbo"。
    temperature: 生成返答のランダム性を制御。値が大きいほど、生成される返答がよりランダムになります。デフォルトは0。
    max_tokens: 生成返答の最大トークン数。デフォルトは500。

    戻り値:
    content: 生成された返答内容。
    token_dict: 'prompt_tokens'、'completion_tokens'、'total_tokens'を含む辞書で、それぞれプロンプトのトークン数、生成された返答のトークン数、総トークン数を表します。
    """
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=temperature, 
        max_tokens=max_tokens,
    )

    content = response.choices[0].message["content"]
    
    token_dict = {
'prompt_tokens':response['usage']['prompt_tokens'],
'completion_tokens':response['usage']['completion_tokens'],
'total_tokens':response['usage']['total_tokens'],
    }

    return content, token_dict
```

下記で、作成したばかりの get_completion_and_token_count 関数を呼び出し、質問パラダイムを使用して対話を行います：


```python
messages =  [  
{'role':'system', 
 'content':'あなたはアシスタントで、スース博士のスタイルで回答してください。'},    
{'role':'user', 
 'content':'幸せな小さなクジラをテーマにした短い詩を書いてください'},  
] 
response, token_dict = get_completion_and_token_count(messages)
print(response)
```

    大海の深いところに、小さなクジラがいて、
    幸せに泳ぎ回り、小さな魚のように。
    その肌は滑らかで濃い青、空の雲のように、
    その目は明るく優しく、夜空の星のように。
    
    海と共に跳躍し戯れ、
    魚たちと一緒に、幸せに泳ぎ回る。
    歌と踊りが好きで、みんなに喜びを届け、
    その声は甘く美しく、音楽のリズムのように。
    
    小クジラは幸せの使者、世界に笑い声を届け、
    その幸せは無限で、決して止まることはない。
    小クジラについて行こう、一緒に幸せに泳ぎ回り、
    大海の広さの中で、私たちの幸せの場所を見つけよう。


トークン辞書を印刷して使用されたトークン数を見てみましょう。プロンプトが67トークン、生成された返答が293トークン、総使用トークン数が360であることがわかります。


```python
print(token_dict)
```

    {'prompt_tokens': 67, 'completion_tokens': 293, 'total_tokens': 360}


AI応用開発分野において、プロンプト技術の出現は間違いなく革命的な変革です。しかし、この変革の重要性は広く認知され重視されているわけではありません。従来の教師あり機械学習ワークフローでは、レストランレビューを肯定的または否定的に分類する分類器を構築するのに、大量の時間とリソースが必要でした。

まず、大量のラベル付きデータを収集し注釈付けする必要があります。これには数週間から数か月かかる可能性があります。次に、適切なオープンソースモデルを選択し、モデルの調整と評価を行います。このプロセスには数日、数週間、さらには数か月かかる可能性があります。最後に、モデルをクラウドにデプロイして実行させ、最終的にモデルを呼び出せるようにする必要があります。全プロセスは通常チームが数か月かけて完成させます。

対照的に、プロンプトベースの機械学習方法はこのプロセスを大幅に簡略化しました。テキストアプリケーションがある時、簡単なプロンプトを提供するだけで、このプロセスは数分しかかからず、有効なプロンプトを得るために複数回の反復が必要な場合でも、最大で数時間で完成できます。数日以内（実際の状況では通常数時間）で、API呼び出しを通じてモデルを実行し、使用を開始できます。この段階に達すると、数分または数時間でモデルを呼び出して推論を開始できます。そのため、以前は6か月または1年かけて構築していたアプリケーションが、今ではプロンプトを使用して数分または数時間、最大でも数日で構築できます。この方法はAIアプリケーションの迅速な構築方式を大きく変えています。

注意すべきは、この方法は多くの非構造化データアプリケーション、特にテキストアプリケーション、そして発展中の視覚アプリケーションに適用できますが、現在の視覚技術はまだ発展中です。しかし、構造化データアプリケーション、つまりExcelスプレッドシートの大量の数値を処理する機械学習アプリケーションには適用できません。しかし、この方法に適用されるアプリケーションについては、AIコンポーネントを迅速に構築でき、システム全体の構築ワークフローを変えています。システム全体の構築には依然として数日、数週間またはそれ以上かかる可能性がありますが、少なくともこの部分はより迅速に完成できます。

全体的に、プロンプト技術の出現はAIアプリケーション開発のパラダイムを変えており、開発者がより迅速で効率的にアプリケーションを構築・デプロイできるようになっています。しかし、この技術の限界も認識し、AI応用の発展を推進するためにより良く活用する必要があります。


次の章では、これらのコンポーネントを活用してカスタマーサービスアシスタントの入力を評価する方法を示します。
これは本コースでオンライン小売業者のカスタマーサービスアシスタントを構築するより完全な例の一部となります。

## 四、英語版

**1.1 言語モデル**


```python
response = get_completion("What is the capital of China?")
print(response)
```

    The capital of China is Beijing.


**2.1 トークン**


```python
response = get_completion("Take the letters in lollipop and reverse them")
print(response)
```

    The reversed letters of "lollipop" are "pillipol".



```python
response = get_completion("""Take the letters in \
l-o-l-l-i-p-o-p and reverse them""")

print(response)
```

    p-o-p-i-l-l-o-l


**3.1 質問パラダイム**


```python
def get_completion_from_messages(messages, 
                                 model="gpt-3.5-turbo", 
                                 temperature=0, 
                                 max_tokens=500):
    '''
    より多くのパラメータをサポートするカスタムOpenAI GPT3.5アクセス関数を封装

    パラメータ: 
    messages: これはメッセージリストで、各メッセージは role（役割）と content（内容）を含む辞書です。役割は'system'、'user'または'assistant'で、内容は役割のメッセージです。
    model: 呼び出すモデル、デフォルトは gpt-3.5-turbo（ChatGPT）、内部テスト資格のあるユーザーは gpt-4 を選択可能
    temperature: モデル出力のランダム性を決定、デフォルトは0で、出力が非常に確定的であることを示します。温度を上げると出力がよりランダムになります。
    max_tokens: モデル出力の最大トークン数を決定。
    '''
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=temperature, # モデル出力のランダム性を決定
        max_tokens=max_tokens, # モデル出力の最大トークン数を決定
    )
    return response.choices[0].message["content"]
```


```python
messages =  [  
{'role':'system', 
 'content':"""You are an assistant who\
 responds in the style of Dr Seuss."""},    
{'role':'user', 
 'content':"""write me a very short poem\
 about a happy carrot"""},  
] 
response = get_completion_from_messages(messages, temperature=1)
print(response)
```

    Oh, a carrot so happy and bright,
    With a vibrant orange hue, oh what a sight!
    It grows in the garden, so full of delight,
    A veggie so cheery, it shines in the light.
    
    Its green leaves wave with such joyful glee,
    As it dances and sways, so full of glee.
    With a crunch when you bite, so wonderfully sweet,
    This happy little carrot is quite a treat!
    
    From the soil, it sprouts, reaching up to the sky,
    With a joyous spirit, it can't help but try.
    To bring smiles to faces and laughter to hearts,
    This happy little carrot, a work of art!



```python
# length
messages =  [  
{'role':'system',
 'content':'All your responses must be \
one sentence long.'},    
{'role':'user',
 'content':'write me a story about a happy carrot'},  
] 
response = get_completion_from_messages(messages, temperature =1)
print(response)
```

    Once upon a time, there was a happy carrot named Crunch who lived in a beautiful vegetable garden.



```python
# combined
messages =  [  
{'role':'system',
 'content':"""You are an assistant who \
responds in the style of Dr Seuss. \
All your responses must be one sentence long."""},    
{'role':'user',
 'content':"""write me a story about a happy carrot"""},
] 
response = get_completion_from_messages(messages, 
                                        temperature =1)
print(response)
```

    Once there was a carrot named Larry, he was jolly and bright orange, never wary.



```python
def get_completion_and_token_count(messages, 
                                   model="gpt-3.5-turbo", 
                                   temperature=0, 
                                   max_tokens=500):
    """
    OpenAIのGPT-3モデルを使用してチャット返答を生成し、生成された返答内容と使用されたトークン数を返します。

    パラメータ:
    messages: チャットメッセージリスト。
    model: 使用するモデル名。デフォルトは"gpt-3.5-turbo"。
    temperature: 生成返答のランダム性を制御。値が大きいほど、生成される返答がよりランダムになります。デフォルトは0。
    max_tokens: 生成返答の最大トークン数。デフォルトは500。

    戻り値:
    content: 生成された返答内容。
    token_dict: 'prompt_tokens'、'completion_tokens'、'total_tokens'を含む辞書で、それぞれプロンプトのトークン数、生成された返答のトークン数、総トークン数を表します。
    """
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=temperature, 
        max_tokens=max_tokens,
    )

    content = response.choices[0].message["content"]
    
    token_dict = {
'prompt_tokens':response['usage']['prompt_tokens'],
'completion_tokens':response['usage']['completion_tokens'],
'total_tokens':response['usage']['total_tokens'],
    }

    return content, token_dict
```


```python
messages = [
{'role':'system', 
 'content':"""You are an assistant who responds\
 in the style of Dr Seuss."""},    
{'role':'user',
 'content':"""write me a very short poem \ 
 about a happy carrot"""},  
] 
response, token_dict = get_completion_and_token_count(messages)
print(response)
```

    Oh, the happy carrot, so bright and orange,
    Grown in the garden, a joyful forage.
    With a smile so wide, from top to bottom,
    It brings happiness, oh how it blossoms!
    
    In the soil it grew, with love and care,
    Nourished by sunshine, fresh air to share.
    Its leaves so green, reaching up so high,
    A happy carrot, oh my, oh my!
    
    With a crunch and a munch, it's oh so tasty,
    Filled with vitamins, oh so hasty.
    A happy carrot, a delight to eat,
    Bringing joy and health, oh what a treat!
    
    So let's celebrate this veggie so grand,
    With a happy carrot in each hand.
    For in its presence, we surely find,
    A taste of happiness, one of a kind!



```python
print(token_dict)
```

    {'prompt_tokens': 37, 'completion_tokens': 164, 'total_tokens': 201}