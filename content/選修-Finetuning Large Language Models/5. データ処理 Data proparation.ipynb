{
	"cells": [
		{
			"cell_type": "markdown",
			"id": "1dfae479-9399-492d-acaa-d9751615ee86",
			"metadata": {
				"id": "1dfae479-9399-492d-acaa-d9751615ee86",
				"tags": []
			},
			"source": [
				"# 第五章 数据处理\n",
				"\n",
				" - [一. 准备训练数据的重要因素](#一.-准备训练数据的重要因素)\n",
				" - [二. 数据处理步骤](#二.-数据处理步骤)\n",
				"     - [2.1 文本 Token 化](#2.1-文本-Token-化)\n",
				"         - [2.1.1 token 化一个文本](#2.1.1-token-化一个文本)\n",
				"         - [2.1.2 一次 token 化多个文本](#2.1.2-一次-token-化多个文本)\n",
				"         - [2.1.3 填充和截断](#2.1.3-填充和截断)\n",
				"         - [2.1.4 准备指令数据集](#2.1.4-准备指令数据集)\n",
				"         - [2.1.5 Token 化一个样本](#2.1.5-Token-化一个样本)\n",
				"         - [2.1.6 Token 化指令数据集](#2.1.6-Token-化指令数据集)\n",
				"     - [2.2 测试/训练数据集的切分](#2.2-测试/训练数据集的切分)\n",
				"         - [2.2.1 一些数据集供您尝试](#2.2.1-一些数据集供您尝试)\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "80133889",
			"metadata": {
				"id": "80133889"
			},
			"source": [
				"在这门课程中，您将学习如何准备训练数据，为您的机器学习模型提供坚实的基础。我们将从数据收集开始，一步步地指导您完成数据预处理、token 化和模型训练的过程。让我们开始吧！\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "YeVJVkc-MbjI",
			"metadata": {
				"id": "YeVJVkc-MbjI"
			},
			"source": [
				"## 一. 准备训练数据的重要因素\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "b32e987c",
			"metadata": {
				"id": "b32e987c"
			},
			"source": [
				"**1. 数据质量**\n",
				"数据质量是数据准备的首要关注点。在微调和训练过程中，高质量的数据能够显著提升模型效果。在准备数据时，请确保提供高质量、准确的输入。有句名言：Garbage in, Garbage out（垃圾进，垃圾出）\n",
				"\n",
				"\n",
				"\n",
				"**2. 数据多样性**\n",
				"数据多样性是另一个至关重要的因素。如果训练数据过于单一，模型可能会过度记忆并在相似情境中重复输出。为了避免这种情况，确保训练数据覆盖各种用例和场景。多样性的数据集有助于让模型更好地理解不同的输入，并做出更准确的预测。\n",
				"\n",
				"\n",
				"\n",
				"**3. 数据真实性**\n",
				"尽管生成数据方法在一些场景中可行，但在大多数情况下，真实数据对于模型的训练和微调非常重要。生成的数据往往具有固定的模式，这可能限制模型的创造力和适应能力。在大多数情况下，拥有真实数据更有效和有帮助，特别是对于写作任务等应用。生成的数据中存在一定的模式。一些服务试图检测内容是否是由生成模型生成的，就是通过寻找生成数据中的模式和规律。\n",
				"\n",
				"\n",
				"\n",
				"\n",
				"**4. 数据数量**\n",
				"数据的数量对于训练模型的确非常重要。更多的数据通常可以帮助模型更好地泛化和适应不同的情境。预训练模型的引入在一定程度上缓解了数据数量的问题，因为它已经通过在互联网上的大量数据上进行预训练，建立了一定的基础理解。因此拥有更多的数据对模型有帮助，但不如前三名那么重要，而且绝对没有质量那么重要。\n",
				"\n",
				"综上所述，准备训练数据时需要考虑数据质量、多样性、真实性和数量。这些因素将共同影响模型的性能和输出质量。接下来，让我们深入了解如何有效地准备训练数据。"
			]
		},
		{
			"cell_type": "markdown",
			"id": "8d83a838",
			"metadata": {
				"id": "8d83a838"
			},
			"source": [
				"## 二. 数据处理步骤\n",
				"\n",
				"**1. 收集指令-响应对**\n",
				"从不同来源收集问答对或指令-响应对。这可以是来自用户的对话记录、已有的问答数据集等。\n",
				"\n",
				"**2. 合并指令对（需要的话，添加 Prompt 模版）**\n",
				"将收集到的指令和响应对进行合并，形成一个整体的数据集。在这一步，可以根据需要为每个指令或响应添加一些 Prompt 模版，以帮助模型更好地理解上下文。\n",
				"\n",
				"**3. token 化**\n",
				"将文本数据转换为数字表示。这一步通过使用分词器（tokenizer）来完成，分词器会将文本划分为词或子词，并为每个词或子词分配一个唯一的标识符，即 token。这样可以将文本转换为机器可以理解的形式，并为后续处理做好准备。在 token 化过程中，还需要进行填充（padding）或截断（truncation）操作，以确保所有文本长度相同，方便模型进行处理。\n",
				"\n",
				"**4. 数据集划分**\n",
				"将处理好的数据集划分为训练集和测试集。训练集用于训练模型的参数，而测试集用于评估模型的性能和泛化能力。\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "bb6a22be",
			"metadata": {
				"id": "bb6a22be"
			},
			"source": [
				"`ing` 是非常常见的一种字符。大部分动名词都存在这个字符。如 finetun**ing** , tokeniz**ing** 中都有 `ing`。这个例子中，`ing` 被编码 为 `278`。\n",
				"\n",
				"当您使用相同的分词器对 token 进行解码时，它会被恢复成原始的文本。\n",
				"\n",
				"每个模型都与特定的分词器相关联，并以此进行训练。如果选择错误的分词器，会导致模型认为不同的数字代表不同的字母集和单词，从而导致混乱和错误的结果。因此，使用正确的分词器至关重要。"
			]
		},
		{
			"cell_type": "markdown",
			"id": "af894033",
			"metadata": {
				"id": "af894033"
			},
			"source": [
				"![tokenizing.png](../../figures/tokenizing.png)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"id": "c333aabd",
			"metadata": {
				"id": "c333aabd"
			},
			"outputs": [],
			"source": [
				"import pandas as pd\n",
				"import datasets\n",
				"\n",
				"from pprint import pprint\n",
				"from transformers import AutoTokenizer"
			]
		},
		{
			"cell_type": "markdown",
			"id": "_0kBBKjNsrML",
			"metadata": {
				"id": "_0kBBKjNsrML"
			},
			"source": [
				"HuggingFace Transformers 库是一个非常强大和受欢迎的自然语言处理工具库。您只需指定所需的模型和名称，它将自动帮助您找到适当的分词器，并与模型进行匹配"
			]
		},
		{
			"cell_type": "markdown",
			"id": "795a30cf",
			"metadata": {
				"id": "795a30cf"
			},
			"source": [
				"### 2.1 文本 Token 化"
			]
		},
		{
			"cell_type": "markdown",
			"id": "JcZOWow4L6s_",
			"metadata": {
				"id": "JcZOWow4L6s_"
			},
			"source": [
				"这里使用 70m pythia 模型对应的分词器"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 52,
			"id": "eb2911fb",
			"metadata": {
				"id": "eb2911fb"
			},
			"outputs": [],
			"source": [
				"tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")"
			]
		},
		{
			"cell_type": "markdown",
			"id": "E0NrDzuIu2G7",
			"metadata": {
				"id": "E0NrDzuIu2G7"
			},
			"source": [
				"#### 2.1.1 token 化一个文本"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"id": "0778167a",
			"metadata": {
				"id": "0778167a"
			},
			"outputs": [],
			"source": [
				"text = \"Hi, how are you?\""
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"id": "emHUBlIHtq44",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "emHUBlIHtq44",
				"outputId": "dd690913-59c6-4ecc-8a24-1b3d9ac065fd"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"[12764, 13, 849, 403, 368, 32]\n"
					]
				}
			],
			"source": [
				"encoded_text = tokenizer(text)[\"input_ids\"]\n",
				"print(encoded_text)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 44,
			"id": "y8Q_Kfa1tivF",
			"metadata": {
				"id": "y8Q_Kfa1tivF"
			},
			"outputs": [],
			"source": [
				"text = \"嗨你好么\""
			]
		},
		{
			"cell_type": "code",
			"execution_count": 45,
			"id": "4b608b9e",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "4b608b9e",
				"outputId": "b876c673-e330-4a77-9cc9-7a2503702b80"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"[161, 234, 103, 24553, 34439, 43244]\n"
					]
				}
			],
			"source": [
				"encoded_text = tokenizer(text)[\"input_ids\"]\n",
				"print(encoded_text)"
			]
		},
		{
			"cell_type": "markdown",
			"id": "5qBAe0P5tEtn",
			"metadata": {
				"id": "5qBAe0P5tEtn"
			},
			"source": [
				"分词器将这串文本编码成了不同的数字。\n",
				"分词器输出一个字典，其中包含表示 token 的`input_ids`"
			]
		},
		{
			"cell_type": "markdown",
			"id": "cciQi04evAHI",
			"metadata": {
				"id": "cciQi04evAHI"
			},
			"source": [
				"现在将其解码回文本"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 7,
			"id": "75f9226a",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "75f9226a",
				"outputId": "573422b9-f031-4ac6-e8ec-fdaa3f71d43d"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Decoded tokens back into text:  Hi, how are you?\n"
					]
				}
			],
			"source": [
				"decoded_text = tokenizer.decode(encoded_text)\n",
				"print(\"Decoded tokens back into text: \", decoded_text)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 46,
			"id": "Jpfzf6v0uWs1",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "Jpfzf6v0uWs1",
				"outputId": "82c66f19-60ac-453a-fe41-d4eac84cfe1c"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"将 token 解码为文本:  嗨你好么\n"
					]
				}
			],
			"source": [
				"decoded_text = tokenizer.decode(encoded_text)\n",
				"print(\"将 token 解码为文本: \", decoded_text)"
			]
		},
		{
			"cell_type": "markdown",
			"id": "EIKNZk6OvG8s",
			"metadata": {
				"id": "EIKNZk6OvG8s"
			},
			"source": [
				"可以看出解码出的文本和最初的一致"
			]
		},
		{
			"cell_type": "markdown",
			"id": "de2602c1",
			"metadata": {
				"id": "de2602c1"
			},
			"source": [
				"#### 2.1.2 一次 token 化多个文本"
			]
		},
		{
			"cell_type": "markdown",
			"id": "JtSAmPiBvXIU",
			"metadata": {
				"id": "JtSAmPiBvXIU"
			},
			"source": [
				"有时候我们需要可以一次性对多个文本进行 token 化。\n",
				"\n",
				"当处理批量输入时，可以将文本列表连接起来作为输入。"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 9,
			"id": "d5be6e7c",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "d5be6e7c",
				"outputId": "f9fb86e9-2137-489a-b710-309b56835711"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Encoded several texts:  [[12764, 13, 849, 403, 368, 32], [42, 1353, 1175], [4374]]\n"
					]
				}
			],
			"source": [
				"list_texts = [\"Hi, how are you?\", \"I'm good\", \"Yes\"]\n",
				"encoded_texts = tokenizer(list_texts)\n",
				"print(\"Encoded several texts: \", encoded_texts[\"input_ids\"])"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 47,
			"id": "VuOAAO1Sv7xg",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "VuOAAO1Sv7xg",
				"outputId": "984c2a4e-6a37-495e-d722-16c5b0ba6c93"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"编码多个文本:  [[161, 234, 103, 24553, 34439, 43244], [15367, 45091, 34439], [12105]]\n"
					]
				}
			],
			"source": [
				"list_texts = [\"嗨你好么\", \"我很好\", \"是\"]\n",
				"encoded_texts = tokenizer(list_texts)\n",
				"print(\"编码多个文本: \", encoded_texts[\"input_ids\"])"
			]
		},
		{
			"cell_type": "markdown",
			"id": "8jj6ZtLqv6iF",
			"metadata": {
				"id": "8jj6ZtLqv6iF"
			},
			"source": [
				"可以看到，分词器对不同长度的文本返回的长度也不一样。"
			]
		},
		{
			"cell_type": "markdown",
			"id": "31cbf826",
			"metadata": {
				"id": "31cbf826"
			},
			"source": [
				"#### 2.1.3 填充和截断"
			]
		},
		{
			"cell_type": "markdown",
			"id": "u9ULVPdj2Mzu",
			"metadata": {
				"id": "u9ULVPdj2Mzu"
			},
			"source": [
				"模型需要处理固定大小的张量，因此在批次中所有内容的长度必须相同。\n",
				"填充是处理这些可变长度编码文本的策略。"
			]
		},
		{
			"cell_type": "markdown",
			"id": "gfbXF0eh2owJ",
			"metadata": {
				"id": "gfbXF0eh2owJ"
			},
			"source": [
				"填充时需要选择一个特定的数字作为填充 token 来代表填充。通常使用 `0` 来填充，这也是句子的结束token。"
			]
		},
		{
			"cell_type": "markdown",
			"id": "_ZKvpmIo2off",
			"metadata": {
				"id": "_ZKvpmIo2off"
			},
			"source": [
				"因此，当我们通过分词器运行 `padding = true` 时，您可以看到 `是(Yes)` 字符串右侧多了很多 `0` 以确保和 `嗨你好么（Hi, how are you?）` 的长度一致。"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 11,
			"id": "88d03447",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "88d03447",
				"outputId": "b67909d5-d6a5-45ab-a76e-d3882f8d8627"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Using padding:  [[12764, 13, 849, 403, 368, 32], [42, 1353, 1175, 0, 0, 0], [4374, 0, 0, 0, 0, 0]]\n"
					]
				}
			],
			"source": [
				"tokenizer.pad_token = tokenizer.eos_token\n",
				"encoded_texts_longest = tokenizer(list_texts, padding=True)\n",
				"print(\"Using padding: \", encoded_texts_longest[\"input_ids\"])"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 48,
			"id": "qrDWBemnQzKU",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "qrDWBemnQzKU",
				"outputId": "a9b12cdf-618b-4e14-d75e-193534471490"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"使用填充后的结果:  [[161, 234, 103, 24553, 34439, 43244], [15367, 45091, 34439, 0, 0, 0], [12105, 0, 0, 0, 0, 0]]\n"
					]
				}
			],
			"source": [
				"tokenizer.pad_token = tokenizer.eos_token\n",
				"encoded_texts_longest = tokenizer(list_texts, padding=True)\n",
				"print(\"使用填充后的结果: \", encoded_texts_longest[\"input_ids\"])"
			]
		},
		{
			"cell_type": "markdown",
			"id": "g4F5tVfN4BPx",
			"metadata": {
				"id": "g4F5tVfN4BPx"
			},
			"source": [
				"模型有着最大长度的限制，即模型可以处理和容纳的文本长度。因此，它并不适用于处理任意长度的文本数据。\n",
				"\n",
				"之前您使用 Prompt 时可能已经注意到，存在着长度限制。这就是模型的截断策略，用于将编码文本截短以适应实际可接受的模型输入。\n",
				"\n",
				"通过截断，我们可以将过长的文本修剪为适合模型处理的长度。这有助于缩短处理时间，因为长文本可能需要更长的处理时间。"
			]
		},
		{
			"cell_type": "markdown",
			"id": "WOetO0R75HPq",
			"metadata": {
				"id": "WOetO0R75HPq"
			},
			"source": [
				"这里将最大长度设置为 3，设置为截断（`truncation = True`）。可以发现`嗨你好么（Hi, how are you?）`短了很多，去掉了右边的所有内容。"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 12,
			"id": "70e1ecfe",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "70e1ecfe",
				"outputId": "34382252-0750-421e-8798-d63dc820a0ff"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Using truncation:  [[12764, 13, 849], [42, 1353, 1175], [4374]]\n"
					]
				}
			],
			"source": [
				"encoded_texts_truncation = tokenizer(list_texts, max_length=3, truncation=True)\n",
				"print(\"Using truncation: \", encoded_texts_truncation[\"input_ids\"])"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 49,
			"id": "bQiZ-xekQ7Qv",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "bQiZ-xekQ7Qv",
				"outputId": "765e6d68-00db-4550-8aae-ccf8252c766f"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"使用截断:  [[24553, 34439, 43244], [15367, 45091, 34439], [12105]]\n"
					]
				}
			],
			"source": [
				"encoded_texts_truncation = tokenizer(list_texts, max_length=3, truncation=True)\n",
				"print(\"使用截断: \", encoded_texts_truncation[\"input_ids\"])"
			]
		},
		{
			"cell_type": "markdown",
			"id": "9AbmHBC65qeI",
			"metadata": {
				"id": "9AbmHBC65qeI"
			},
			"source": [
				"在实际应用中，比如您正在撰写一篇文章，可能在某个位置给出了一些 prompt，同时还有许多重要的内容需要考虑。保留右侧可能更重要，这保存了前文的重要信息，以保持上下文的连贯性。这时，将截断的一侧标定为左侧可能会更加合适。所以，最终的决定取决于您正在解决的具体问题和所需的结果。"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 13,
			"id": "e7de95f5",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "e7de95f5",
				"outputId": "b8a7e8e5-803b-4512-b5ad-7c809b427c9d"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Using left-side truncation:  [[403, 368, 32], [42, 1353, 1175], [4374]]\n"
					]
				}
			],
			"source": [
				"tokenizer.truncation_side = \"left\"\n",
				"encoded_texts_truncation_left = tokenizer(list_texts, max_length=3, truncation=True)\n",
				"print(\"Using left-side truncation: \", encoded_texts_truncation_left[\"input_ids\"])"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 50,
			"id": "pCMTiaKhRAXn",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "pCMTiaKhRAXn",
				"outputId": "73fbb564-0aaa-4947-ccab-f175d8768068"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"使用左侧截断:  [[24553, 34439, 43244], [15367, 45091, 34439], [12105]]\n"
					]
				}
			],
			"source": [
				"tokenizer.truncation_side = \"left\"\n",
				"encoded_texts_truncation_left = tokenizer(list_texts, max_length=3, truncation=True)\n",
				"print(\"使用左侧截断: \", encoded_texts_truncation_left[\"input_ids\"])"
			]
		},
		{
			"cell_type": "markdown",
			"id": "QvfMy2026_KM",
			"metadata": {
				"id": "QvfMy2026_KM"
			},
			"source": [
				"实际上，我们在处理输入时常常同时使用填充和截断这两种方法。我们设置截断和填充的参数都为 True。可以看到填充为 0 的内容被截断为三个。\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 14,
			"id": "6dd95990",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "6dd95990",
				"outputId": "9f365518-00be-43f1-bc4d-c85b5f6ce15c"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Using both padding and truncation:  [[403, 368, 32], [42, 1353, 1175], [4374, 0, 0]]\n"
					]
				}
			],
			"source": [
				"encoded_texts_both = tokenizer(list_texts, max_length=3, truncation=True, padding=True)\n",
				"print(\"Using both padding and truncation: \", encoded_texts_both[\"input_ids\"])"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 51,
			"id": "YGsCLjHpRF9y",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "YGsCLjHpRF9y",
				"outputId": "c3b6de3b-df3f-4a60-d0f3-5bbbadecefdb"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"同时使用填充和截断:  [[24553, 34439, 43244], [15367, 45091, 34439], [12105, 0, 0]]\n"
					]
				}
			],
			"source": [
				"encoded_texts_both = tokenizer(list_texts, max_length=3, truncation=True, padding=True)\n",
				"print(\"同时使用填充和截断: \", encoded_texts_both[\"input_ids\"])"
			]
		},
		{
			"cell_type": "markdown",
			"id": "591152e5",
			"metadata": {
				"id": "591152e5"
			},
			"source": [
				"#### 2.1.4 准备指令数据集"
			]
		},
		{
			"cell_type": "markdown",
			"id": "d0a3cac5",
			"metadata": {
				"id": "d0a3cac5"
			},
			"source": [
				"下面是上一个实验中的一些代码。\n",
				"\n",
				"加载带有 \"question\" 和 \"answer\" 的数据集文件，将其放入 Prompt 中进行处理。\n",
				"\n",
				"现在您可以在此处看到一个包含 \"question\" 和 \"answer\" 的数据。\n",
				"\n",
				"我们在其中一个数据上运行这个 token 生成器。\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 15,
			"id": "95b406b6",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "95b406b6",
				"outputId": "a8f6683e-c41e-45e0-d1b9-51577aa7a4ff"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"One datapoint in the finetuning dataset:\n",
						"{'answer': 'Lamini has documentation on Getting Started, Authentication, '\n",
						"           'Question Answer Model, Python Library, Batching, Error Handling, '\n",
						"           'Advanced topics, and class documentation on LLM Engine available '\n",
						"           'at https://lamini-ai.github.io/.',\n",
						" 'question': '### Question:\\n'\n",
						"             'What are the different types of documents available in the '\n",
						"             'repository (e.g., installation guide, API documentation, '\n",
						"             \"developer's guide)?\\n\"\n",
						"             '\\n'\n",
						"             '### Answer:'}\n"
					]
				}
			],
			"source": [
				"import pandas as pd\n",
				"\n",
				"filename = \"lamini_docs.jsonl\"\n",
				"instruction_dataset_df = pd.read_json(filename, lines=True)\n",
				"examples = instruction_dataset_df.to_dict()\n",
				"\n",
				"if \"question\" in examples and \"answer\" in examples:\n",
				"  text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
				"elif \"instruction\" in examples and \"response\" in examples:\n",
				"  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
				"elif \"input\" in examples and \"output\" in examples:\n",
				"  text = examples[\"input\"][0] + examples[\"output\"][0]\n",
				"else:\n",
				"  text = examples[\"text\"][0]\n",
				"\n",
				"prompt_template = \"\"\"### Question:\n",
				"{question}\n",
				"\n",
				"### Answer:\"\"\"\n",
				"\n",
				"num_examples = len(examples[\"question\"])\n",
				"finetuning_dataset = []\n",
				"for i in range(num_examples):\n",
				"  question = examples[\"question\"][i]\n",
				"  answer = examples[\"answer\"][i]\n",
				"  text_with_prompt_template = prompt_template.format(question=question)\n",
				"  finetuning_dataset.append({\"question\": text_with_prompt_template, \"answer\": answer})\n",
				"\n",
				"from pprint import pprint\n",
				"print(\"One datapoint in the finetuning dataset:\") #微调数据集中的一个数据点\n",
				"pprint(finetuning_dataset[0])"
			]
		},
		{
			"cell_type": "markdown",
			"id": "72e7e805",
			"metadata": {
				"id": "72e7e805"
			},
			"source": [
				"#### 2.1.5 Token 化一个样本"
			]
		},
		{
			"cell_type": "markdown",
			"id": "30GwKNRe-LlQ",
			"metadata": {
				"id": "30GwKNRe-LlQ"
			},
			"source": [
				"\n",
				"首先将该问题与该答案连接起来，然后通过分词器进行 token 化。\n",
				"\n",
				"为了简单起见，这里只是将张量作为 NumPy 数组返回，并且进行填充操作。\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 16,
			"id": "5175d1b1",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "5175d1b1",
				"outputId": "fc5ab43a-e332-4328-ad2a-66d8a33ca364"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"[[ 4118 19782    27   187  1276   403   253  1027  3510   273  7177  2130\n",
						"    275   253 18491   313    70    15    72   904 12692  7102    13  8990\n",
						"  10097    13 13722   434  7102  6177   187   187  4118 37741    27    45\n",
						"   4988    74   556 10097   327 27669 11075   264    13  5271 23058    13\n",
						"  19782 37741 10031    13 13814 11397    13   378 16464    13 11759 10535\n",
						"   1981    13 21798 12989    13   285   966 10097   327 21708    46 10797\n",
						"   2130   387  5987  1358    77  4988    74    14  2284    15  7280    15\n",
						"    900 14206]]\n"
					]
				}
			],
			"source": [
				"text = finetuning_dataset[0][\"question\"] + finetuning_dataset[0][\"answer\"]\n",
				"tokenized_inputs = tokenizer(\n",
				"    text,\n",
				"    return_tensors=\"np\",\n",
				"    padding=True\n",
				")\n",
				"print(tokenized_inputs[\"input_ids\"])"
			]
		},
		{
			"cell_type": "markdown",
			"id": "dVp2iO73-LaM",
			"metadata": {
				"id": "dVp2iO73-LaM"
			},
			"source": [
				"因为不确定这些 token 的实际长度。\n",
				"\n",
				"所以，将配置的最大长度设置为最大长度和 token 长度的最小值。\n",
				"\n",
				"当然，您总是可以将填充长度设置为最大长度。"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 17,
			"id": "3240b6b2",
			"metadata": {
				"id": "3240b6b2"
			},
			"outputs": [],
			"source": [
				"max_length = 2048\n",
				"max_length = min(\n",
				"    tokenized_inputs[\"input_ids\"].shape[1],\n",
				"    max_length,\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"id": "EHAB-Hqs_rir",
			"metadata": {
				"id": "EHAB-Hqs_rir"
			},
			"source": [
				"然后，再次对其进行 token 化，并将其截断为最大长度。"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 18,
			"id": "cac7a18a",
			"metadata": {
				"id": "cac7a18a"
			},
			"outputs": [],
			"source": [
				"tokenized_inputs = tokenizer(\n",
				"    text,\n",
				"    return_tensors=\"np\",\n",
				"    truncation=True,\n",
				"    max_length=max_length\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 19,
			"id": "f3dbd287",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "f3dbd287",
				"outputId": "78385553-38aa-44d4-bcec-53981076f6a9"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"[[ 4118 19782    27   187  1276   403   253  1027  3510   273  7177  2130\n",
						"    275   253 18491   313    70    15    72   904 12692  7102    13  8990\n",
						"  10097    13 13722   434  7102  6177   187   187  4118 37741    27    45\n",
						"   4988    74   556 10097   327 27669 11075   264    13  5271 23058    13\n",
						"  19782 37741 10031    13 13814 11397    13   378 16464    13 11759 10535\n",
						"   1981    13 21798 12989    13   285   966 10097   327 21708    46 10797\n",
						"   2130   387  5987  1358    77  4988    74    14  2284    15  7280    15\n",
						"    900 14206]]\n"
					]
				}
			],
			"source": [
				"print(tokenized_inputs[\"input_ids\"])"
			]
		},
		{
			"cell_type": "markdown",
			"id": "ebcb8c75",
			"metadata": {
				"id": "ebcb8c75"
			},
			"source": [
				"#### 2.1.6 Token 化指令数据集"
			]
		},
		{
			"cell_type": "markdown",
			"id": "1_iISc4mAQKa",
			"metadata": {
				"id": "1_iISc4mAQKa"
			},
			"source": [
				"将上述过程包装成一个函数，便于在整个数据集上运行它。\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 20,
			"id": "908f0485",
			"metadata": {
				"id": "908f0485"
			},
			"outputs": [],
			"source": [
				"def tokenize_function(examples):\n",
				"    \"\"\"\n",
				"    对输入进行 token 化，并进行填充和截断处理，返回经过处理后的 token 作为结果\n",
				"\n",
				"    Args:\n",
				"        examples (dict): 待 token 化的数据，可以是包含\"question\"和\"answer\"键的字典，或包含\"input\"和\"output\"键的字典，或包含\"text\"键的字典\n",
				"\n",
				"    Returns:\n",
				"        dict: 经过处理后的输入数据的 token，包含经过 token 化并进行填充和截断处理后的输入数据\n",
				"    \"\"\"\n",
				"\n",
				"    # 合并指令对\n",
				"    if \"question\" in examples and \"answer\" in examples:\n",
				"      text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
				"    elif \"input\" in examples and \"output\" in examples:\n",
				"      text = examples[\"input\"][0] + examples[\"output\"][0]\n",
				"    else:\n",
				"      text = examples[\"text\"][0]\n",
				"\n",
				"    # token 化\n",
				"    tokenizer.pad_token = tokenizer.eos_token\n",
				"    tokenized_inputs = tokenizer(\n",
				"        text,\n",
				"        return_tensors=\"np\",\n",
				"        padding=True,\n",
				"    )\n",
				"\n",
				"    max_length = min(\n",
				"        tokenized_inputs[\"input_ids\"].shape[1],\n",
				"        2048\n",
				"    )\n",
				"    tokenizer.truncation_side = \"left\"\n",
				"    tokenized_inputs = tokenizer(\n",
				"        text,\n",
				"        return_tensors=\"np\",\n",
				"        truncation=True,\n",
				"        max_length=max_length\n",
				"    )\n",
				"\n",
				"    return tokenized_inputs"
			]
		},
		{
			"cell_type": "markdown",
			"id": "W6OY3UYK9sfm",
			"metadata": {
				"id": "W6OY3UYK9sfm"
			},
			"source": [
				"\n",
				"现在我们加载该数据集。\n",
				"\n",
				"\n",
				"我们使用 map 方法将 token 化函数映射到该数据集。\n",
				"\n",
				"我们将 batch_size 设置为1，这样可以进行分批处理。\n",
				"\n",
				"将 drop_last_batch 设置为 True，以处理混合大小的输入，因为当数据长度不是  batch_size 的倍数时，最后一个 batch 的 size 会与 batch_size 不同。\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 30,
			"id": "04ef80c4",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/",
					"height": 120,
					"referenced_widgets": [
						"cd8509a93c9f4c7bb46bfebf0cfd0e48",
						"59516bee277041b0b740c3888075fc34",
						"9b35fa6fe47d425baa8d0a8007cdc56a",
						"f06b46b61ebe4a12934a58436b71628a",
						"a84b7fb90c86409ea0fde265016cb088",
						"16be33567f7e4f748e1e65d399fe6f68",
						"fc2c70ebd25f4de9b3b0f198ebf144d8",
						"f370a17ceb104fe3b67206e4451f4479",
						"92db973050e04cc0bf186782c620db39",
						"57908b6719774ad39f7a8ed79575a0f1",
						"dd19399a96ce417e87ed6f09e8883323"
					]
				},
				"id": "04ef80c4",
				"outputId": "7f6170e6-53b0-41e6-add4-525bf1713454"
			},
			"outputs": [
				{
					"data": {
						"application/vnd.jupyter.widget-view+json": {
							"model_id": "cd8509a93c9f4c7bb46bfebf0cfd0e48",
							"version_major": 2,
							"version_minor": 0
						},
						"text/plain": [
							"Map:   0%|          | 0/1400 [00:00<?, ? examples/s]"
						]
					},
					"metadata": {},
					"output_type": "display_data"
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Dataset({\n",
						"    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
						"    num_rows: 1400\n",
						"})\n"
					]
				}
			],
			"source": [
				"finetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=filename, split=\"train\")\n",
				"\n",
				"tokenized_dataset = finetuning_dataset_loaded.map(\n",
				"    tokenize_function,\n",
				"    batched=True,\n",
				"    batch_size=1,\n",
				"    drop_last_batch=True\n",
				")\n",
				"\n",
				"print(tokenized_dataset)"
			]
		},
		{
			"cell_type": "markdown",
			"id": "6Ju3KbXrDeHh",
			"metadata": {
				"id": "6Ju3KbXrDeHh"
			},
			"source": [
				"添加标签列，以便模型进行学习\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 22,
			"id": "e984c2a0",
			"metadata": {
				"id": "e984c2a0"
			},
			"outputs": [],
			"source": [
				"tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"
			]
		},
		{
			"cell_type": "markdown",
			"id": "176fec6c",
			"metadata": {
				"id": "176fec6c"
			},
			"source": [
				"### 2.2 测试/训练数据集的切分"
			]
		},
		{
			"cell_type": "markdown",
			"id": "uBDrZU_NAbj7",
			"metadata": {
				"id": "uBDrZU_NAbj7"
			},
			"source": [
				"运行这个训练测试分割函数，将指定测试大小为数据的10%。\n",
				"\n",
				"当然，您可以根据数据集的大小来更改此设置。\n",
				"\n",
				"`shuffle=True` 为了随机化这个数据集的顺序。\n",
				"\n",
				"现在可以看到数据集已分为训练集和测试集。\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 23,
			"id": "c2d0b28d",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "c2d0b28d",
				"outputId": "d4d84cc1-b0f1-42f8-8d42-22182c0f7efc"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"DatasetDict({\n",
						"    train: Dataset({\n",
						"        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
						"        num_rows: 1260\n",
						"    })\n",
						"    test: Dataset({\n",
						"        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
						"        num_rows: 140\n",
						"    })\n",
						"})\n"
					]
				}
			],
			"source": [
				"split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
				"print(split_dataset)"
			]
		},
		{
			"cell_type": "markdown",
			"id": "b8b3d16e",
			"metadata": {
				"id": "b8b3d16e"
			},
			"source": [
				"#### 2.2.1 一些数据集供您尝试"
			]
		},
		{
			"cell_type": "markdown",
			"id": "T_hSmMUfEI15",
			"metadata": {
				"id": "T_hSmMUfEI15"
			},
			"source": [
				"\n",
				"我们使用的数据可以直接通过 Hugging Face 加载。\n",
				"\n",
				"该数据集是关于一家公司的专业数据集，也许这与您的公司相似。\n",
				"\n",
				"您可以根据需要对其进行调整。"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 29,
			"id": "835c1848",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "835c1848",
				"outputId": "bb33571c-b60d-40a8-e2fe-1b848558bf80"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"DatasetDict({\n",
						"    train: Dataset({\n",
						"        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
						"        num_rows: 1260\n",
						"    })\n",
						"    test: Dataset({\n",
						"        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
						"        num_rows: 140\n",
						"    })\n",
						"})\n"
					]
				}
			],
			"source": [
				"finetuning_dataset_path = \"lamini/lamini_docs\"\n",
				"finetuning_dataset = datasets.load_dataset(finetuning_dataset_path)\n",
				"print(finetuning_dataset)"
			]
		},
		{
			"cell_type": "markdown",
			"id": "JcK-fpjS99XJ",
			"metadata": {
				"id": "JcK-fpjS99XJ"
			},
			"source": [
				"如果您觉得这个数据集有些无聊。\n",
				"\n",
				"我们提供了一些更有趣的数据集供您选择。\n",
				"1. `taylor_swift`的数据集，\n",
				"2. 流行乐队 `BTS` 的数据集。\n",
				"3. 实际的开源大型语言模型数据集。\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 25,
			"id": "0ad8af28",
			"metadata": {
				"id": "0ad8af28"
			},
			"outputs": [],
			"source": [
				"taylor_swift_dataset = \"lamini/taylor_swift\"\n",
				"bts_dataset = \"lamini/bts\"\n",
				"open_llms = \"lamini/open_llms\""
			]
		},
		{
			"cell_type": "markdown",
			"id": "Ve7IZUogEdDs",
			"metadata": {
				"id": "Ve7IZUogEdDs"
			},
			"source": [
				"\n",
				"现在让我们来看看`taylor_swift`数据集中的一条数据，好的。\n",
				"\n",
				"这些数据集同样可以通过 Hugging Face 获得。"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 28,
			"id": "d8ffb3a7",
			"metadata": {
				"colab": {
					"base_uri": "https://localhost:8080/"
				},
				"id": "d8ffb3a7",
				"outputId": "d81db7b2-50eb-4c34-caa5-a2cf42624c74"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"{'question': 'What is the most popular Taylor Swift song among millennials? How does this song relate to the millennial generation? What is the significance of this song in the millennial culture?', 'answer': 'Taylor Swift\\'s \"Shake It Off\" is the most popular song among millennials. This song relates to the millennial generation as it is an anthem of self-acceptance and embracing one\\'s individuality. The song\\'s message of not letting others bring you down and to just dance it off resonates with the millennial culture, which is often characterized by a strong sense of individuality and a rejection of societal norms. Additionally, the song\\'s upbeat and catchy melody makes it a perfect fit for the millennial generation, which is known for its love of pop music.', 'input_ids': [1276, 310, 253, 954, 4633, 11276, 24619, 4498, 2190, 24933, 8075, 32, 1359, 1057, 436, 4498, 14588, 281, 253, 24933, 451, 5978, 32, 1737, 310, 253, 8453, 273, 436, 4498, 275, 253, 24933, 451, 4466, 32, 37979, 24619, 434, 346, 2809, 640, 733, 5566, 3, 310, 253, 954, 4633, 4498, 2190, 24933, 8075, 15, 831, 4498, 7033, 281, 253, 24933, 451, 5978, 347, 352, 310, 271, 49689, 273, 1881, 14, 14764, 593, 285, 41859, 581, 434, 2060, 414, 15, 380, 4498, 434, 3935, 273, 417, 13872, 2571, 3324, 368, 1066, 285, 281, 816, 11012, 352, 745, 8146, 684, 342, 253, 24933, 451, 4466, 13, 534, 310, 2223, 7943, 407, 247, 2266, 3282, 273, 2060, 414, 285, 247, 18235, 273, 38058, 22429, 15, 9157, 13, 253, 4498, 434, 598, 19505, 285, 5834, 90, 40641, 2789, 352, 247, 3962, 4944, 323, 253, 24933, 451, 5978, 13, 534, 310, 1929, 323, 697, 2389, 273, 1684, 3440, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1276, 310, 253, 954, 4633, 11276, 24619, 4498, 2190, 24933, 8075, 32, 1359, 1057, 436, 4498, 14588, 281, 253, 24933, 451, 5978, 32, 1737, 310, 253, 8453, 273, 436, 4498, 275, 253, 24933, 451, 4466, 32, 37979, 24619, 434, 346, 2809, 640, 733, 5566, 3, 310, 253, 954, 4633, 4498, 2190, 24933, 8075, 15, 831, 4498, 7033, 281, 253, 24933, 451, 5978, 347, 352, 310, 271, 49689, 273, 1881, 14, 14764, 593, 285, 41859, 581, 434, 2060, 414, 15, 380, 4498, 434, 3935, 273, 417, 13872, 2571, 3324, 368, 1066, 285, 281, 816, 11012, 352, 745, 8146, 684, 342, 253, 24933, 451, 4466, 13, 534, 310, 2223, 7943, 407, 247, 2266, 3282, 273, 2060, 414, 285, 247, 18235, 273, 38058, 22429, 15, 9157, 13, 253, 4498, 434, 598, 19505, 285, 5834, 90, 40641, 2789, 352, 247, 3962, 4944, 323, 253, 24933, 451, 5978, 13, 534, 310, 1929, 323, 697, 2389, 273, 1684, 3440, 15]}\n"
					]
				}
			],
			"source": [
				"dataset_swiftie = datasets.load_dataset(taylor_swift_dataset)\n",
				"print(dataset_swiftie[\"train\"][1])"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 27,
			"id": "c3789bbd",
			"metadata": {
				"id": "c3789bbd"
			},
			"outputs": [],
			"source": [
				"# 以下是将自己的数据集推送到 Huggingface hub 的方法\n",
				"# !pip install huggingface_hub\n",
				"# !huggingface-cli login\n",
				"# split_dataset.push_to_hub(dataset_path_hf)"
			]
		},
		{
			"cell_type": "markdown",
			"id": "kZ_Ao7pF9yBc",
			"metadata": {
				"id": "kZ_Ao7pF9yBc"
			},
			"source": [
				"我们已经准备好了所有的数据，并进行了 token 化。在接下来的实验中，我们将使用这些数据来训练我们的模型。"
			]
		}
	],
	"metadata": {
		"colab": {
			"provenance": []
		},
		"kernelspec": {
			"display_name": "Python 3.9.6 64-bit",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.6.9"
		},
		"vscode": {
			"interpreter": {
				"hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
			}
		}
	},
	"nbformat": 4,
	"nbformat_minor": 5
}