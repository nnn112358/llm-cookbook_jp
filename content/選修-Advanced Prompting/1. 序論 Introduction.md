# 第一章 高度プロンプト手法チュートリアル 序論


👨‍💻👨‍💻 貢献者：曾浩龙（Datawhale 意向成員）和邹雨衡（Datawhale 成員）

🎉🎉 本チュートリアルへようこそ。ここでは、基礎理論とコード実装を含む、最先端のプロンプト手法について深く探求します。重要な原理を詳細に解析し、実際のコード例を提供することで、これらの手法を包括的に理解し習得していただけるよう支援します。

🚀🚀 本チュートリアルは、Datawhaleが開発者向けに提供するLLM実践コースの重要な構成要素です。


## 1. チュートリアル概要


> 人工知能に抑圧されるのではなく、人工知能によって力を与えられよう。

大規模言語モデル（Large Language Models, LLMs）のプロンプトエンジニアリング（Prompt Engineering）とは、高品質なプロンプトを精巧に設計することで、LLMsがより正確で信頼性があり期待に沿った出力内容を生成するよう導き、より効率的でインテリジェントな情報処理と意思決定を実現することを指します。

Datawhaleは開発者向けのChatGPTプロンプトエンジニアリングの日本語コースを提供しており、このコースは[吴恩达](https://www.andrewng.org/)と[伊萨 <span style="color:black">&#8226;</span> 福尔福德](https://twitter.com/isafulf)の[ChatGPT Prompt Engineering for Developers](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)コースをベースに作成されています。前述のコースは初心者向けに、プロンプトの構築方法を分かりやすく紹介し、OpenAI のAPIを使用して要約、推論、変換などの一般的な機能を実装する方法を説明しており、これはLLM開発を学ぶ第一歩です。しかし、プロンプトエンジニアリングを深く研究し、LLMsの潜在力を十分に活用したい場合、これだけでは十分ではありません。

🥳🥳 そのため、本チュートリアルでは、最先端のプロンプト手法の基礎理論とコード実装の紹介に焦点を当てます。重要な原理を深く解析し、実際のコード例を提供することで、読者がこれらの最先端プロンプト手法を包括的に理解し習得できるよう支援します。本チュートリアルを完了後、読者はこれらの高度なプロンプト方法を専門的かつ正確に理解し応用できるようになります。**本チュートリアルの主な内容は以下の通りです**：

- **思考の連鎖と自己一貫性**。思考の連鎖（Chain-of-Thought, CoT）の本質は離散的プロンプト学習の一種です。一連の中間推論ステップを生成することで、LLMsが複雑な推論を実行する能力を向上させます。思考の連鎖プロンプトの例が少ない場合や入手困難な場合、ゼロショット思考の連鎖が威力を発揮します。自己一貫性は思考の連鎖の構築を拡張し、複数の思考の連鎖を生成し、多数決投票によって最終回答を選択します。自己一貫性戦略は、複雑な推論問題は通常、異なる複数の思考方式を通じて同一の正しい答えに到達できるという理念を活用しています。

- **プロンプト最適化ツール**。LLMを活用して入力プロンプトを最適化します。通常、高品質な回答を生成する効果的なプロンプトを作成するために、いくつかの原則と戦略に従う必要があります。実際に、これらの原則と戦略を利用してプロンプトを効果的に最適化することも可能です。

- **協調推論と行動 ReAct**。ReAct = Reason + Action = 推論 + 行動。ReAct手法は、LLMに特定のタスクに関連する推論テキストを生成するよう導くことで、検索やツール呼び出しのアクションをトリガーし、質問応答タスクの性能を向上させます。

- **感情刺激プロンプト**。LLMsは心理的感情刺激を理解することで、特定の質問応答タスクでの性能を向上させる可能性があります。感情刺激系プロンプトはシンプルで使いやすく、多くの場合において試してみる価値があり注目に値します。

- **ステップバックプロンプト Step-Back Prompting**。「ステップバックプロンプト」と呼ばれるシンプルなプロンプト技術により、LLMsは抽象化を行い、特定の詳細を含む実例から高レベルの概念と重要な原理を抽象化できるようになります。得られた高レベルの概念と重要な原理を使用して推論を導くことで、LLMsは正しい推論パスに従う能力を大幅に向上させました。

- **プログラム支援LLM**。PAL: **P**rogram-**a**ided **L**anguage Models はPythonプログラムを中間推論ステップとして使用し、解決と計算のタスクを外部のPythonインタープリターにアウトソーシングし、言語モデル自体に完全に依存するのではありません。

- **思考骨格プロンプト**。Skeleton of Thought（SoT）は順序通りに答えを生成するのではなく、答えの異なる部分を並列的に生成します。より具体的には、問題が与えられた際、SoTはまずLLMを導いて思考骨格（複数の思考要点）を構築し、その後バッチデコーディングまたは並列API呼び出しを実行して複数の要点を並列的に拡張し、最後に出力結果を統合して最終答案を得ます。


## 2. その他の説明


📢📢 本チュートリアルを作成する過程で、比較的有名なプロンプト手法の中で含めなかったものがいくつかあります：

- 生成知識プロンプト。論文：[ACL 2022 - Generated Knowledge Prompting for Commonsense Reasoning](https://arxiv.org/abs/2110.08387)。この手法は言語モデルから知識を生成し、質問に答える際にこれらの知識を追加の入力として使用します。これはLLMが常識推論を改善するための柔軟な外部知識ソースとして機能できることを示しています。含めなかった理由：ステップバックプロンプト（Step-Back Prompting）はより高度な手法であり、その動作原理は背景知識と基礎原理を生成して推論を促進することと類似しています。

- チェーン式プロンプト。チェーン式プロンプト（[Prompt Chaining](https://docs.anthropic.com/claude/docs/prompt-chaining)）はタスクを複数のサブタスクに分解する手法です。サブタスクを決定した後、サブタスクのプロンプトをLLMに提供し、得られた結果を新しいプロンプトの一部として使用します。含めなかった理由：タスクを明確に定義されたサブタスクに分解することは時々困難であり、また優良なプロンプトチェーンの例を作成する難易度も問題によって異なります。さらに、初期のプロンプトに対する応答が誤っている場合、その後のプロンプトで混乱を引き起こす可能性があります。

- いくつかの自動プロンプトエンジニアリングの実装手法が既に存在します。例えば：[APE](https://openreview.net/forum?id=92gvk82DE-)、[AutoPrompt](https://aclanthology.org/2020.emnlp-main.346.pdf)、[PromptBreeder](https://arxiv.org/pdf/2309.16797.pdf)、[PromptAgent](https://arxiv.org/abs/2310.16427)などです。含めなかった理由：想像していたほど汎用的、効果的、使いやすいものではなく、特定のタスクに対して検索と最適化を行う必要があり、現在も研究段階にあります。

- [Active Prompting](https://arxiv.org/pdf/2302.12246.pdf)。思考の連鎖を利用してLLMを能動的にプロンプトする手法です。最初のステップは、少数のCoT例を使用してLLMにクエリを行うかどうかを選択することです。その後、一組の訓練問題に対してk個の可能な答えを生成し、これらk個の答えに基づいて不確実性（不一致性を使用）を計算します。次に、最も不確実な問題を選択して人間がアノテーションを行い、新しいアノテーション例を使用して各問題を推論します。含めなかった理由：思考の連鎖推論に基づいた改善が少しあり、時々人間が設計したCoT推論のアノテーションが必要になり、これは面倒です。

- [最少から最多へのプロンプト（Least to Most Prompting, LtM）](https://openreview.net/forum?id=WZH7099tgfM)。これは思考の連鎖プロンプトプロセス（CoT Prompting）をさらに発展させたもので、思考の連鎖プロンプトプロセスと同様に、解決すべき問題を相互に築き上げられた一組のサブ問題に分解します。第二ステップでは、これらのサブ問題を一つずつ解決します。思考の連鎖と異なるのは、前のサブ問題の解決策がプロンプトに入力され、次の問題を解決しようとすることです。含めなかった理由：CoTと類似しているため。

- [方向性刺激プロンプト](https://arxiv.org/pdf/2302.11520.pdf)。ブラックボックスLLMsを特定の期待される出力方向に導く新しいフレームワークです。この手法はLLMを直接調整するのではなく、調整可能な戦略LMを訓練して、各入力インスタンスに対して補助的な方向性刺激プロンプトを生成します。これらのプロンプトは、細かく特定のインスタンスに合わせたプロンプトやヒントとして機能し、LLMが必要な結果を生成するよう導きます。例えば、生成された要約に特定のキーワードを含めるなどです。含めなかった理由：実装が複雑で使いにくいため。

- いくつかの非常に複雑なプロンプト戦略。例えば[ToT（Tree of Thoughts）](https://arxiv.org/abs/2305.10601)、[PoT（Program of Thoughts）](https://openreview.net/forum?id=YfZ4ZPt8zd)、[GoT（Graph of Thoughts）](https://arxiv.org/abs/2308.09687)、[AoT（Algorithm of Thoughts）](https://arxiv.org/abs/2308.10379)などです。含めなかった理由：現在、これらの手法は幅広い適用性を持たず、実装プロセスが非常に複雑です。これらは主に推論能力の向上方法を研究し、特定のベンチマークテストで最高スコアを更新するために使用されています。GPT-3.5 TurboやGPT-4が既に得意とする多くの既存タスク、例えばクリエイティブライティング、知識問答、テキスト要約、感情分析、機械翻訳などにおいて、ToTのような意図的な検索は必要ない可能性があります。


💻💻 実験コードのプログラミング環境説明：

- `Anaconda + Python 3.8.10`（Pythonのバージョン要件はPython 3.7+）

- 主要な依存ライブラリ：`openai==1.10.0`、`langchain==0.1.5`、`langchain-experimental==0.0.50`、`langchain-openai==0.0.5`、`numexpr==2.8.6`、`google-search-results==2.4.2`

- LLM：`gpt-3.5-turbo-0125`


📚📚 事前知識：

- LLMsに関連する基礎概念の理解。例えばプロンプト学習、プロンプトエンジニアリング、指示遵守、ゼロショット/フューショットプロンプト、コンテキスト学習、検索拡張生成など。

- OpenAI APIに関する知識。キー、重要なパラメーター、メッセージタイプなどを含み、Python openaiライブラリを熟練して使用できること。


