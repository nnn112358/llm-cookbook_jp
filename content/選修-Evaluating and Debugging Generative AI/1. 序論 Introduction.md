# 生成AI の評価とデバッグ

## 序論

コース「生成AI の評価とデバッグ」へようこそ 👏🏻👏🏻

本コースは Carey Phelps（Weights & Biases チーフプロダクトマネージャー）と Deeplearning.ai の協力により開発され、生成AI モデルを効果的に追跡・デバッグするための体系的な手法とツールの提供を目指しています。

機械学習システムの構築過程において、すべてのデータ、モデル、ハイパーパラメータの管理と追跡は複雑になる可能性があります。チーム規模の拡大に伴い、この複雑さはさらに増大する可能性があります。生成AI モデルは、その出力が複雑であるため評価が困難であり、教師あり学習モデルと比較して複雑さの層を追加します。

本コースでは、Weights & Biases のツールを使用します。これは使いやすく柔軟なツールセットで、機械学習実験追跡の業界標準となっています。本コースは、テキスト生成のための大規模言語モデルと画像生成のための拡散モデルをカバーします。


## コース内容

本コースでは、以下の内容に焦点を当てます：

1. 実験の追跡と可視化方法
2. 拡散モデルの監視方法
3. 大規模言語モデル（LLMs）の評価と微調整方法

以下を含む、一連のデバッグと評価ツールを学習します：

- Experiments：機械学習実験の追跡用
- Artifacts：データセットとモデルのバージョン管理と保存用
- Tables：モデルが行った予測の可視化と検査用
- Reports：実験結果の協働と共有用
- Model Registry：モデルのライフサイクル管理用
- Prompts：大規模言語モデル生成の評価用

これらのツールは、Python、TensorFlow、PyTorch などの主流フレームワークや計算プラットフォームと連携できます。

## 謝辞

Weights & Biases の Darek Kleczek と Thomas Capelle、および Deeplearning.ai の Geoff Ludwig と Tommy Nelson の本コースへの貢献に特に感謝いたします。

本コース完了後、生成AI の評価とデバッグのベストプラクティスを理解し、生成AI プロジェクトの体系的評価とデバッグのためのツールセットを習得することができます。皆様の参加をお待ちしており、本コースから利益を得て、機械学習プロジェクトのより大きな成功を推進できることを願っています。